{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669e9b8f",
   "metadata": {},
   "source": [
    "## 17-4. 내가 원하는 숫자 이미지 만들기 (1) Generator 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df08ddb",
   "metadata": {},
   "source": [
    "### 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thisisiron/TF2-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42debec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-dataset (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-dataset\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e57033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      3\u001b[0m mnist, info \u001b[38;5;241m=\u001b[39m  tfds\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m fig \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mshow_examples(mnist, info)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist, info =  tfds.load(\n",
    "    \"mnist\", split=\"train\", with_info=True\n",
    ")\n",
    "\n",
    "fig = tfds.show_examples(mnist, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def gan_preprocessing(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def cgan_preprocessing(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    \n",
    "    label = tf.one_hot(data[\"label\"], 10)\n",
    "    return image, label\n",
    "\n",
    "gan_datasets = mnist.map(gan_preprocessing).shuffle(1000).batch(BATCH_SIZE)\n",
    "cgan_datasets = mnist.map(cgan_preprocessing).shuffle(100).batch(BATCH_SIZE)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba997b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i,j in cgan_datasets : break\n",
    "\n",
    "# 이미지 i와 라벨 j가 일치하는지 확인해 봅니다.     \n",
    "print(\"Label :\", j[0])\n",
    "print(\"Image Min/Max :\", i.numpy().min(), i.numpy().max())\n",
    "plt.imshow(i.numpy()[0,...,0], plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e5e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63f28f6d",
   "metadata": {},
   "source": [
    "#### GAN Generator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "\n",
    "class GeneratorGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(GeneratorGAN, self).__init__()\n",
    "\n",
    "        self.dense_1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_2 = layers.Dense(256, activation='relu')\n",
    "        self.dense_3 = layers.Dense(512, activation='relu')\n",
    "        self.dense_4 = layers.Dense(28*28*1, activation='tanh')\n",
    "\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "\n",
    "    def call(self, noise):\n",
    "        out = self.dense_1(noise)\n",
    "        out = self.dense_2(out)\n",
    "        out = self.dense_3(out)\n",
    "        out = self.dense_4(out)\n",
    "        return self.reshape(out)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7968e71",
   "metadata": {},
   "source": [
    "#### cGAN Generator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2955f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(GeneratorCGAN, self).__init__()\n",
    "        \n",
    "        self.dense_z = layers.Dense(256, activation='relu')\n",
    "        self.dense_y = layers.Dense(256, activation='relu')\n",
    "        self.combined_dense = layers.Dense(512, activation='relu')\n",
    "        self.final_dense = layers.Dense(28 * 28 * 1, activation='tanh')\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "\n",
    "    def call(self, noise, label):\n",
    "        noise = self.dense_z(noise)\n",
    "        label = self.dense_y(label)\n",
    "        out = self.combined_dense(tf.concat([noise, label], axis=-1))\n",
    "        out = self.final_dense(out)\n",
    "        return self.reshape(out)\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b57e6c",
   "metadata": {},
   "source": [
    "## 17-5. 내가 원하는 숫자 이미지 만들기 (2) Discriminator 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386c688",
   "metadata": {},
   "source": [
    "#### GAN Discriminator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorGAN, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.blocks = []\n",
    "        for f in [512, 256, 128, 1]:\n",
    "            self.blocks.append(\n",
    "                layers.Dense(f, activation=None if f==1 else \"relu\")\n",
    "            )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87c75b",
   "metadata": {},
   "source": [
    "#### cGAN Discriminator 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87604a",
   "metadata": {},
   "source": [
    "cGAN의 Discriminator는 Maxout이라는레이어가 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa0eb1",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/e-25-4-1.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ff8df",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/e-25-4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fded7",
   "metadata": {},
   "source": [
    "2개의 fully-connected 레이어를 사용할 때 Maxout을 식으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234f5a7",
   "metadata": {},
   "source": [
    "$$max(w_1^Tx+b_1,\\ w_2^Tx+b_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48636a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxout(layers.Layer):\n",
    "    def __init__(self, units, pieces):\n",
    "        super(Maxout, self).__init__()\n",
    "        self.dense = layers.Dense(units*pieces, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(.5)    \n",
    "        self.reshape = layers.Reshape((-1, pieces, units))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.reshape(x)\n",
    "        return tf.math.reduce_max(x, axis=2)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1125cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440718e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorCGAN, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.image_block = Maxout(240, 5)\n",
    "        self.label_block = Maxout(50, 5)\n",
    "        self.combine_block = Maxout(240, 4)\n",
    "        \n",
    "        self.dense = layers.Dense(1, activation=None)\n",
    "    \n",
    "    def call(self, image, label):\n",
    "        image = self.flatten(image)\n",
    "        image = self.image_block(image)\n",
    "        label = self.label_block(label)\n",
    "        x = layers.Concatenate()([image, label])\n",
    "        x = self.combine_block(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde38560",
   "metadata": {},
   "source": [
    "## 17-6. 내가 원하는 숫자 이미지 만들기 (3) 학습 및 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19678293",
   "metadata": {},
   "source": [
    "#### GAN, cGAN 각각의 모델 학습에 공통적으로 필요한 loss function과 optimizer를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b308618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses\n",
    "\n",
    "bce = losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return bce(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "gene_opt = optimizers.Adam(1e-4)\n",
    "disc_opt = optimizers.Adam(1e-4)    \n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d118c86",
   "metadata": {},
   "source": [
    "#### GAN으로 MNIST 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_generator = GeneratorGAN()\n",
    "gan_discriminator = DiscriminatorGAN()\n",
    "\n",
    "@tf.function()\n",
    "def gan_step(real_images):\n",
    "    noise = tf.random.normal([real_images.shape[0], 100])\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator를 이용해 가짜 이미지 생성\n",
    "        fake_images = gan_generator(noise)\n",
    "        # Discriminator를 이용해 진짜 및 가짜이미지를 각각 판별\n",
    "        real_out = gan_discriminator(real_images)\n",
    "        fake_out = gan_discriminator(fake_images)\n",
    "        # 각 손실(loss)을 계산\n",
    "        gene_loss = generator_loss(fake_out)\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "    # gradient 계산\n",
    "    gene_grad = tape.gradient(gene_loss, gan_generator.trainable_variables)\n",
    "    disc_grad = tape.gradient(disc_loss, gan_discriminator.trainable_variables)\n",
    "    # 모델 학습\n",
    "    gene_opt.apply_gradients(zip(gene_grad, gan_generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad, gan_discriminator.trainable_variables))\n",
    "    return gene_loss, disc_loss\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for i, images in enumerate(gan_datasets):\n",
    "        gene_loss, disc_loss = gan_step(images)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch}/{EPOCHS} EPOCHS, {i+1} ITER] G:{gene_loss}, D:{disc_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1533010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "noise = tf.random.normal([10, 100])\n",
    "\n",
    "output = gan_generator(noise)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ~/aiffel/conditional_generation/gan\n",
    "# !cp ~/data/gan/GAN_500.zip ~/aiffel/conditional_generation/gan/\n",
    "# !cd ~/aiffel/conditional_generation/gan && unzip GAN_500.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터 불러오기\n",
    "\n",
    "import os\n",
    "weight_path = os.getenv('HOME')+'/aiffel/conditional_generation/gan/GAN_500'\n",
    "\n",
    "noise = tf.random.normal([10, 100]) \n",
    "\n",
    "gan_generator = GeneratorGAN()\n",
    "gan_generator.load_weights(weight_path)\n",
    "\n",
    "output = gan_generator(noise)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5f74e",
   "metadata": {},
   "source": [
    "#### cGAN으로 MNIST 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19badd",
   "metadata": {},
   "source": [
    "위에서 실행했던 GAN 학습처럼 약간의 학습으로는 제대로 된 생성 결과를 얻을 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_generator = GeneratorCGAN()\n",
    "cgan_discriminator = DiscriminatorCGAN()\n",
    "\n",
    "@tf.function()\n",
    "def cgan_step(real_images, labels):\n",
    "    noise = tf.random.normal([real_images.shape[0], 100])\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_images = cgan_generator(noise, labels)\n",
    "        \n",
    "        real_out = cgan_discriminator(real_images, labels)\n",
    "        fake_out = cgan_discriminator(fake_images, labels)\n",
    "        \n",
    "        gene_loss = generator_loss(fake_out)\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "    \n",
    "    gene_grad = tape.gradient(gene_loss, cgan_generator.trainable_variables)\n",
    "    disc_grad = tape.gradient(disc_loss, cgan_discriminator.trainable_variables)\n",
    "    \n",
    "    gene_opt.apply_gradients(zip(gene_grad, cgan_generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad, cgan_discriminator.trainable_variables))\n",
    "    return gene_loss, disc_loss\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(cgan_datasets):\n",
    "        gene_loss, disc_loss = cgan_step(images, labels)\n",
    "    \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch}/{EPOCHS} EPOCHS, {i} ITER] G:{gene_loss}, D:{disc_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec07dd",
   "metadata": {},
   "source": [
    "시간을 아끼기 위해 위 코드로 500 epoch 학습한 가중치를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a63412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ~/aiffel/conditional_generation/cgan\n",
    "# !cp ~/data/cgan/CGAN_500.zip ~/aiffel/conditional_generation/cgan/\n",
    "# !cd ~/aiffel/conditional_generation/cgan && unzip CGAN_500.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1277684",
   "metadata": {},
   "outputs": [],
   "source": [
    "number =  0 # TODO : 생성할 숫자를 입력해 주세요!!\n",
    "\n",
    "weight_path = os.getenv('HOME')+'/aiffel/conditional_generation/cgan/CGAN_500'\n",
    "\n",
    "noise = tf.random.normal([10, 100])\n",
    "\n",
    "label = tf.one_hot(number, 10)\n",
    "label = tf.expand_dims(label, axis=0)\n",
    "label = tf.repeat(label, 10, axis=0)\n",
    "\n",
    "generator = GeneratorCGAN()\n",
    "generator.load_weights(weight_path)\n",
    "\n",
    "output = generator(noise, label)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b713b",
   "metadata": {},
   "source": [
    "## 17-7. GAN의 입력에 이미지를 넣는다면? Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a98c8",
   "metadata": {},
   "source": [
    "Pix2Pix는 이미지를 입력으로 하여 원하는 다른 형태의 이미지로 변환시킬 수 있는 GAN 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6a8cc",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1611.07004.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb37b70",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_results.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38dfb2f",
   "metadata": {},
   "source": [
    "노이즈와 레이블 정보를 함께 입력했던 cGAN은 fully-connected 레이어를 연속적으로 쌓아 만들었지만, 이미지 변환이 목적인 Pix2Pix는 이미지를 다루는데 효율적인 convolution 레이어를 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05743f64",
   "metadata": {},
   "source": [
    "#### Pix2Pix (Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753b89a",
   "metadata": {},
   "source": [
    "이미지를 입력받아 변환된 이미지를 출력 - 입력 이미지와 변환된 이미지의 크기는 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253554a",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_generator.max-800x600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb1cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93905989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232229ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "U-Net 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9491105",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_generator_unet.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537e5b2",
   "metadata": {},
   "source": [
    "Encoder와 Decoder가 연결(skip connection)되어있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3a331",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_result_loss.max-800x600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed53a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d5000fc",
   "metadata": {},
   "source": [
    "https://medium.com/@msmapark2/u-net-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-u-net-convolutional-networks-for-biomedical-image-segmentation-456d6901b28a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809671f",
   "metadata": {},
   "source": [
    "L1 이라 쓰여있는 Generator만으로 생성된 결과는 매우 흐릿합니다. 이미지를 변환하는데 L1(MAE)이나 L2(MSE) 손실만을 이용해서 학습하는 경우 이렇게 결과가 흐릿해지는 경향이 있습니다. Generator가 단순히 이미지의 평균적인 손실만을 줄이고자 파라미터를 학습하기 때문에 이러한 현상이 불가피합니다.\n",
    "\n",
    "반면 위 그림의 cGAN이라 쓰여진 GAN 기반의 학습 방법은 비교적 훨씬 더 세밀한 정보를 잘 표현하고 있습니다. Discriminator를 잘 속이려면 Generator가 (Ground truth라고 쓰여진 이미지같이) 진짜 같은 이미지를 만들어야 하기 때문이죠. 논문에서는 L1손실과 GAN 손실을 같이 사용하면 더욱더 좋은 결과를 얻을 수 있다고 합니다 (위 그림의 L1+cGAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135eae4",
   "metadata": {},
   "source": [
    "#### Pix2Pix (Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62e048",
   "metadata": {},
   "source": [
    "DCGAN의 Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a575f",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/dcgan_d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf9362",
   "metadata": {},
   "source": [
    "DCGAN의 Discriminator는 진위만 판별"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca414c",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/patchgan.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a24ec",
   "metadata": {},
   "source": [
    "Pix2Pix에서 사용되는 Discriminator는 하나의 값이 아닌 여러 개의 값으로 진위여부를 판단\n",
    "\n",
    "PatchGAN - 서로 다른 영역에 대해 진짜/가짜를 나타내는 여러 개의 확률 값을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51249b48",
   "metadata": {},
   "source": [
    "#### 판별 영역을 다양한 크기로 실험한결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12edcc",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/patchgan_results.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9474b0",
   "metadata": {},
   "source": [
    "일반적으로 이미지에서 거리가 먼 두 픽셀은 서로 연관성이 거의 없기 때문에 특정 크기를 가진 일부 영역에 대해서 세부적으로 진짜/가짜를 판별하는 것이 Generator로 하여금 더 진짜 같은 이미지를 만들게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1263c8b",
   "metadata": {},
   "source": [
    "## 17-8. 난 스케치를 할 테니 너는 채색을 하거라 (1) 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831aff82",
   "metadata": {},
   "source": [
    "데이터셋은 Sketch2Pokemon이라는 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fc6dd",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/norod78/sketch2pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc71976",
   "metadata": {},
   "source": [
    "학습용 데이터 셋에 830개의 이미지가 있으며, 각 (256x256) 크기의 이미지 쌍이 나란히 붙어 (256x512) 크기의 이미지로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ~/aiffel/conditional_generation/data\n",
    "# !ln -s ~/data/sketch2pokemon.zip ~/aiffel/conditional_generation/data\n",
    "# !cd ~/aiffel/conditional_generation/data && unzip sketch2pokemon.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.getenv('HOME')+'/aiffel/conditional_generation/data/pokemon_pix2pix_dataset/train/'\n",
    "print(\"number of train examples :\", len(os.listdir(data_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5512809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "for i in range(1, 7):\n",
    "    f = data_path + os.listdir(data_path)[np.random.randint(800)]\n",
    "    img = cv2.imread(f, cv2.IMREAD_COLOR)\n",
    "    plt.subplot(3,2,i)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2f1ef",
   "metadata": {},
   "source": [
    "스케치 생성 모델을 이용해서 만든 그림들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1968b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = data_path + os.listdir(data_path)[0]\n",
    "img = cv2.imread(f, cv2.IMREAD_COLOR)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a58a2",
   "metadata": {},
   "source": [
    " 첫 번째 스케치를 다음 단계에서 구성할 Pix2Pix 모델에 입력하여 두 번째 그림과 같은 채색된 이미지를 생성하는 것이 이번 단계의 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2097f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def normalize(x):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    return (x/127.5) - 1\n",
    "\n",
    "def denormalize(x):\n",
    "    x = (x+1)*127.5\n",
    "    x = x.numpy()\n",
    "    return x.astype(np.uint8)\n",
    "\n",
    "def load_img(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_image(img, 3)\n",
    "    \n",
    "    w = tf.shape(img)[1] // 2\n",
    "    sketch = img[:, :w, :] \n",
    "    sketch = tf.cast(sketch, tf.float32)\n",
    "    colored = img[:, w:, :] \n",
    "    colored = tf.cast(colored, tf.float32)\n",
    "    return normalize(sketch), normalize(colored)\n",
    "\n",
    "f = data_path + os.listdir(data_path)[1]\n",
    "sketch, colored = load_img(f)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(1,2,1); plt.imshow(denormalize(sketch))\n",
    "plt.subplot(1,2,2); plt.imshow(denormalize(colored))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14e1b0",
   "metadata": {},
   "source": [
    "데이터의 다양성을 높이기 위해 아래 코드와 같이 여러 augmentation 방법을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import image\n",
    "from tensorflow.keras.preprocessing.image import random_rotation\n",
    "\n",
    "@tf.function() # 빠른 텐서플로 연산을 위해 @tf.function()을 사용합니다. \n",
    "def apply_augmentation(sketch, colored):\n",
    "    stacked = tf.concat([sketch, colored], axis=-1)\n",
    "    \n",
    "    _pad = tf.constant([[30,30],[30,30],[0,0]])\n",
    "    if tf.random.uniform(()) < .5:\n",
    "        padded = tf.pad(stacked, _pad, \"REFLECT\")\n",
    "    else:\n",
    "        padded = tf.pad(stacked, _pad, \"CONSTANT\", constant_values=1.)\n",
    "\n",
    "    out = image.random_crop(padded, size=[256, 256, 6])\n",
    "    \n",
    "    out = image.random_flip_left_right(out)\n",
    "    out = image.random_flip_up_down(out)\n",
    "    \n",
    "    if tf.random.uniform(()) < .5:\n",
    "        degree = tf.random.uniform([], minval=1, maxval=4, dtype=tf.int32)\n",
    "        out = image.rot90(out, k=degree)\n",
    "    \n",
    "    return out[...,:3], out[...,3:]   \n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae473e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,13))\n",
    "img_n = 1\n",
    "for i in range(1, 13, 2):\n",
    "    augmented_sketch, augmented_colored = apply_augmentation(sketch, colored)\n",
    "    \n",
    "    plt.subplot(3,4,i)\n",
    "    plt.imshow(denormalize(augmented_sketch)); plt.title(f\"Image {img_n}\")\n",
    "    plt.subplot(3,4,i+1); \n",
    "    plt.imshow(denormalize(augmented_colored)); plt.title(f\"Image {img_n}\")\n",
    "    img_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2998bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data\n",
    "\n",
    "def get_train(img_path):\n",
    "    sketch, colored = load_img(img_path)\n",
    "    sketch, colored = apply_augmentation(sketch, colored)\n",
    "    return sketch, colored\n",
    "\n",
    "train_images = data.Dataset.list_files(data_path + \"*.jpg\")\n",
    "train_images = train_images.map(get_train).shuffle(100).batch(4)\n",
    "\n",
    "sample = train_images.take(1)\n",
    "sample = list(sample.as_numpy_iterator())\n",
    "sketch, colored = (sample[0][0]+1)*127.5, (sample[0][1]+1)*127.5\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.imshow(sketch[0].astype(np.uint8))\n",
    "plt.subplot(1,2,2); plt.imshow(colored[0].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f428b00",
   "metadata": {},
   "source": [
    "## 17-9. 난 스케치를 할 테니 너는 채색을 하거라 (2) Generator 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b9baa",
   "metadata": {},
   "source": [
    "Generator의 구성요소 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873b854",
   "metadata": {},
   "source": [
    "pix2pix 논문에서 Generator를 구성하는데 필요한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5734224",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/refer_g.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4770a6",
   "metadata": {},
   "source": [
    "입출력 크기를 주의 깊게 보기 바람"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c0225",
   "metadata": {},
   "source": [
    "Generator 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "\n",
    "class EncodeBlock(layers.Layer):\n",
    "    def __init__(self, n_filters, use_bn=True):\n",
    "        super(EncodeBlock, self).__init__()\n",
    "        self.use_bn = use_bn       \n",
    "        self.conv = layers.Conv2D(n_filters, 4, 2, \"same\", use_bias=False)\n",
    "        self.batchnorm = layers.BatchNormalization()\n",
    "        self.lrelu= layers.LeakyReLU(0.2)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "        return self.lrelu(x)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        filters = [64,128,256,512,512,512,512,512]\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i, f in enumerate(filters):\n",
    "            if i == 0:\n",
    "                self.blocks.append(EncodeBlock(f, use_bn=False))\n",
    "            else:\n",
    "                self.blocks.append(EncodeBlock(f))\n",
    "    \n",
    "    def call(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "    def get_summary(self, input_shape=(256,256,3)):\n",
    "        inputs = Input(input_shape)\n",
    "        return Model(inputs, self.call(inputs)).summary()\n",
    "\n",
    "print(\"✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder().get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bdff5",
   "metadata": {},
   "source": [
    " Decoder를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeBlock(layers.Layer):\n",
    "    def __init__(self, f, dropout=True):\n",
    "        super(DecodeBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.Transconv = layers.Conv2DTranspose(f, 4, 2, \"same\", use_bias=False)\n",
    "        self.batchnorm = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.Transconv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        if self.dropout:\n",
    "            x = layers.Dropout(.5)(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        filters = [512,512,512,512,256,128,64]\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i, f in enumerate(filters):\n",
    "            if i < 3:\n",
    "                self.blocks.append(DecodeBlock(f))\n",
    "            else:\n",
    "                self.blocks.append(DecodeBlock(f, dropout=False))\n",
    "                \n",
    "        self.blocks.append(layers.Conv2DTranspose(3, 4, 2, \"same\", use_bias=False))\n",
    "        \n",
    "    def call(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "            \n",
    "    def get_summary(self, input_shape=(1,1,512)):\n",
    "        inputs = Input(input_shape)\n",
    "        return Model(inputs, self.call(inputs)).summary()\n",
    "        \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder().get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9247f5f",
   "metadata": {},
   "source": [
    "tf.keras.Model을 상속받아 Encoder와 Decoder를 연결해 Generator를 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderGenerator(Model):\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoderGenerator, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "   \n",
    "    def get_summary(self, input_shape=(256,256,3)):\n",
    "        inputs = Input(input_shape)\n",
    "        return Model(inputs, self.call(inputs)).summary()\n",
    "        \n",
    "\n",
    "EncoderDecoderGenerator().get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaede96b",
   "metadata": {},
   "source": [
    "## 17-10. 난 스케치를 할 테니 너는 채색을 하거라 (3) Generator 재구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00506c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncodeBlock(layers.Layer):\n",
    "#     def __init__(self, n_filters, use_bn=True):\n",
    "#         super(EncodeBlock, self).__init__()\n",
    "#         self.use_bn = use_bn       \n",
    "#         self.conv = layers.Conv2D(n_filters, 4, 2, \"same\", use_bias=False)\n",
    "#         self.batchnorm = layers.BatchNormalization()\n",
    "#         self.lrelu = layers.LeakyReLU(0.2)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         if self.use_bn:\n",
    "#             x = self.batchnorm(x)\n",
    "#         return self.lrelu(x)\n",
    "\n",
    "    \n",
    "# class DecodeBlock(layers.Layer):\n",
    "#     def __init__(self, f, dropout=True):\n",
    "#         super(DecodeBlock, self).__init__()\n",
    "#         self.dropout = dropout\n",
    "#         self.Transconv = layers.Conv2DTranspose(f, 4, 2, \"same\", use_bias=False)\n",
    "#         self.batchnorm = layers.BatchNormalization()\n",
    "#         self.relu = layers.ReLU()\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         x = self.Transconv(x)\n",
    "#         x = self.batchnorm(x)\n",
    "#         if self.dropout:\n",
    "#             x = layers.Dropout(.5)(x)\n",
    "#         return self.relu(x)\n",
    "    \n",
    "# print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570470c",
   "metadata": {},
   "source": [
    "U-Net Generator를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(Model):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        encode_filters = [64,128,256,512,512,512,512,512]\n",
    "        decode_filters = [512,512,512,512,256,128,64]\n",
    "        \n",
    "        self.encode_blocks = []\n",
    "        for i, f in enumerate(encode_filters):\n",
    "            if i == 0:\n",
    "                self.encode_blocks.append(EncodeBlock(f, use_bn=False))\n",
    "            else:\n",
    "                self.encode_blocks.append(EncodeBlock(f))\n",
    "        \n",
    "        self.decode_blocks = []\n",
    "        for i, f in enumerate(decode_filters):\n",
    "            if i < 3:\n",
    "                self.decode_blocks.append(DecodeBlock(f))\n",
    "            else:\n",
    "                self.decode_blocks.append(DecodeBlock(f, dropout=False))\n",
    "        \n",
    "        self.last_conv = layers.Conv2DTranspose(3, 4, 2, \"same\", use_bias=False)\n",
    "    \n",
    "    def call(self, x):\n",
    "        features = []\n",
    "        for block in self.encode_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        features = features[:-1]\n",
    "                    \n",
    "        for block, feat in zip(self.decode_blocks, features[::-1]):\n",
    "            x = block(x)\n",
    "            x = layers.Concatenate()([x, feat])\n",
    "        \n",
    "        x = self.last_conv(x)\n",
    "        return x\n",
    "                \n",
    "    def get_summary(self, input_shape=(256,256,3)):\n",
    "        inputs = Input(input_shape)\n",
    "        return Model(inputs, self.call(inputs)).summary()\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4da1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974eb724",
   "metadata": {},
   "source": [
    "## 17-11. 난 스케치를 할 테니 너는 채색을 하거라 (4) Discriminator 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad16628",
   "metadata": {},
   "source": [
    "Discriminator의 구성요소 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc57d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a5b9c3",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/refer_d.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4240091b",
   "metadata": {},
   "source": [
    " 최종 출력 크기가 (30,30,1)이 되어야 하는 이유는 앞서 Discriminator에 대해 알아봤던 70x70 PatchGAN을 사용했기 때문입니다. 최종 (30,30) 출력에서 각 픽셀의 receptive field 크기를 (70,70)으로 맞추기 위해 Discriminator의 출력 크기를 (30,30) 크기로 강제로 맞추는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0afbc",
   "metadata": {},
   "source": [
    "https://sahiltinky94.medium.com/understanding-patchgan-9f3c8380c207"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd20662",
   "metadata": {},
   "source": [
    "Discriminator 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e32590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscBlock(layers.Layer):\n",
    "    def __init__(self, n_filters, stride=2, custom_pad=False, use_bn=True, act=True):\n",
    "        super(DiscBlock, self).__init__()\n",
    "        self.custom_pad = custom_pad\n",
    "        self.use_bn = use_bn\n",
    "        self.act = act\n",
    "        \n",
    "        if custom_pad:\n",
    "            self.padding = layers.ZeroPadding2D()\n",
    "            self.conv = layers.Conv2D(n_filters, 4, stride, \"valid\", use_bias=False)\n",
    "        else:\n",
    "            self.conv = layers.Conv2D(n_filters, 4, stride, \"same\", use_bias=False)\n",
    "        \n",
    "        self.batchnorm = layers.BatchNormalization() if use_bn else None\n",
    "        self.lrelu = layers.LeakyReLU(0.2) if act else None\n",
    "        \n",
    "    def call(self, x):\n",
    "        if self.custom_pad:\n",
    "            x = self.padding(x)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "                \n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x)\n",
    "            \n",
    "        if self.act:\n",
    "            x = self.lrelu(x)\n",
    "        return x \n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ff67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((128,128,32))\n",
    "out = layers.ZeroPadding2D()(inputs)\n",
    "out = layers.Conv2D(64, 4, 1, \"valid\", use_bias=False)(out)\n",
    "out = layers.BatchNormalization()(out)\n",
    "out = layers.LeakyReLU(0.2)(out)\n",
    "\n",
    "Model(inputs, out).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4f633",
   "metadata": {},
   "source": [
    "Pix2Pix의 Discriminator가 70x70 PatchGAN을 사용하기 때문에 최종 출력을 (30,30) 크기로 맞추느라 위와 같이 조금 복잡한 과정을 거침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.block1 = layers.Concatenate()\n",
    "        self.block2 = DiscBlock(n_filters=64, stride=2, custom_pad=False, use_bn=False, act=True)\n",
    "        self.block3 = DiscBlock(n_filters=128, stride=2, custom_pad=False, use_bn=True, act=True)\n",
    "        self.block4 = DiscBlock(n_filters=256, stride=2, custom_pad=False, use_bn=True, act=True)\n",
    "        self.block5 = DiscBlock(n_filters=512, stride=1, custom_pad=True, use_bn=True, act=True)\n",
    "        self.block6 = DiscBlock(n_filters=1, stride=1, custom_pad=True, use_bn=False, act=False)\n",
    "        self.sigmoid = layers.Activation(\"sigmoid\")\n",
    "        \n",
    "        # filters = [64,128,256,512,1]\n",
    "        # self.blocks = [layers.Concatenate()]\n",
    "        # for i, f in enumerate(filters):\n",
    "        #     self.blocks.append(DiscBlock(\n",
    "        #         n_filters=f,\n",
    "        #         strides=2 if i<3 else 1,\n",
    "        #         custom_pad=False if i<3 else True,\n",
    "        #         use_bn=False if i==0 and i==4 else True,\n",
    "        #         act=True if i<4 else False\n",
    "        #     ))\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        out = self.block1([x, y])\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.block5(out)\n",
    "        out = self.block6(out)\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "    def get_summary(self, x_shape=(256,256,3), y_shape=(256,256,3)):\n",
    "        x, y = Input(x_shape), Input(y_shape) \n",
    "        return Model((x, y), self.call(x, y)).summary()\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Discriminator().get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc38c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([1,256,256,3])\n",
    "y = tf.random.uniform([1,256,256,3])\n",
    "\n",
    "disc_out = Discriminator()(x, y)\n",
    "plt.imshow(disc_out[0, ... ,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cbf9e",
   "metadata": {},
   "source": [
    "## 17-12. 난 스케치를 할 테니 너는 채색을 하거라 (5) 학습 및 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f83e5",
   "metadata": {},
   "source": [
    "손실 함수 선택에 따른 결과의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49591f3f",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_result_loss2.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7748f2f",
   "metadata": {},
   "source": [
    "일반적인 GAN의 손실 함수에 L1을 추가로 이용했을 때 결과가 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f750fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "bce = losses.BinaryCrossentropy(from_logits=False)\n",
    "mae = losses.MeanAbsoluteError()\n",
    "\n",
    "def get_gene_loss(fake_output, real_output, fake_disc):\n",
    "    l1_loss = mae(real_output, fake_output)\n",
    "    gene_loss = bce(tf.ones_like(fake_disc), fake_disc)\n",
    "    return gene_loss, l1_loss\n",
    "\n",
    "def get_disc_loss(fake_disc, real_disc):\n",
    "    return bce(tf.zeros_like(fake_disc), fake_disc) + bce(tf.ones_like(real_disc), real_disc)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b7732",
   "metadata": {},
   "source": [
    "Generator의 손실 함수 (위 코드의 get_gene_loss)는 총 3개의 입력이 있습니다. 이 중 fake_disc는 Generator가 생성한 가짜 이미지를 Discriminator에 입력하여 얻어진 값이며, 실제 이미지를 뜻하는 \"1\"과 비교하기 위해 tf.ones_like()를 사용합니다. 또한 L1 손실을 계산하기 위해 생성한 가짜 이미지(fake_output)와 실제 이미지(real_output) 사이의 MAE(Mean Absolute Error)를 계산합니다.\n",
    "\n",
    "Discriminator의 손실 함수 (위 코드의 get_disc_loss)는 2개의 입력이 있으며, 이들은 가짜 및 진짜 이미지가 Discriminator에 각각 입력되어 얻어진 값입니다. Discriminator는 실제 이미지를 잘 구분해 내야 하므로 real_disc는 \"1\"로 채워진 벡터와 비교하고, fake_disc는 \"0\"으로 채워진 벡터와 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "gene_opt = optimizers.Adam(2e-4, beta_1=.5, beta_2=.999)\n",
    "disc_opt = optimizers.Adam(2e-4, beta_1=.5, beta_2=.999)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sketch, real_colored):\n",
    "    with tf.GradientTape() as gene_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generator 예측\n",
    "        fake_colored = generator(sketch, training=True)\n",
    "        # Discriminator 예측\n",
    "        fake_disc = discriminator(sketch, fake_colored, training=True)\n",
    "        real_disc = discriminator(sketch, real_colored, training=True)\n",
    "        # Generator 손실 계산\n",
    "        gene_loss, l1_loss = get_gene_loss(fake_colored, real_colored, fake_disc)\n",
    "        gene_total_loss = gene_loss + (100 * l1_loss) ## <===== L1 손실 반영 λ=100\n",
    "        # Discrminator 손실 계산\n",
    "        disc_loss = get_disc_loss(fake_disc, real_disc)\n",
    "                \n",
    "    gene_gradient = gene_tape.gradient(gene_total_loss, generator.trainable_variables)\n",
    "    disc_gradient = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    gene_opt.apply_gradients(zip(gene_gradient, generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_gradient, discriminator.trainable_variables))\n",
    "    return gene_loss, l1_loss, disc_loss\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221894a9",
   "metadata": {},
   "source": [
    "논문에서는 Generator의 손실을 아래와 같이 정의했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96291515",
   "metadata": {},
   "source": [
    "$$G^* = \\arg \\min_G \\max_D \\mathcal{L}_{cGAN}(G,D) + \\lambda \\mathcal{L}_{L1}(G)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c0ee3",
   "metadata": {},
   "source": [
    "λλ는 학습 과정에서 L1 손실을 얼마나 반영할 것인지를 나타내며 논문에서는 λ=100λ=100을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "generator = UNetGenerator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for i, (sketch, colored) in enumerate(train_images):\n",
    "        g_loss, l1_loss, d_loss = train_step(sketch, colored)\n",
    "                \n",
    "        # 10회 반복마다 손실을 출력합니다.\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"EPOCH[{epoch}] - STEP[{i+1}] \\\n",
    "                    \\nGenerator_loss:{g_loss.numpy():.4f} \\\n",
    "                    \\nL1_loss:{l1_loss.numpy():.4f} \\\n",
    "                    \\nDiscriminator_loss:{d_loss.numpy():.4f}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ind = 1\n",
    "\n",
    "f = data_path + os.listdir(data_path)[test_ind]\n",
    "sketch, colored = load_img(f)\n",
    "\n",
    "pred = generator(tf.expand_dims(sketch, 0))\n",
    "pred = denormalize(pred)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1); plt.imshow(denormalize(sketch))\n",
    "plt.subplot(1,3,2); plt.imshow(pred[0])\n",
    "plt.subplot(1,3,3); plt.imshow(denormalize(colored))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10cd34",
   "metadata": {},
   "source": [
    " 이렇게 Encoder-Decoder Generator, U-Net Generator, Discriminator의 구현했음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308a9ae",
   "metadata": {},
   "source": [
    "## 17-13. 프로젝트 : Segmentation map으로 도로 이미지 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c58a61",
   "metadata": {},
   "source": [
    "도로의 레이블 정보를 활용해 이미지를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743c52b",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/p2p_result_seg.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041440f2",
   "metadata": {},
   "source": [
    "https://d3s0tskafalll9.cloudfront.net/media/documents/cityscapes.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058cf5dc",
   "metadata": {},
   "source": [
    "1000개의 학습용 이미지 및 5개의 평가 이미지를 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e97e60",
   "metadata": {},
   "source": [
    "Tensroflow에서 제공하는 Pix2Pix 튜토리얼은 위 이미지와 비슷한 레이블 정보 이미지를 사용하기 때문에 좋은 참고 자료가 될 수 있을 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80dc654",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/generative/pix2pix?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0c1e5",
   "metadata": {},
   "source": [
    "프로젝트 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38ce80",
   "metadata": {},
   "source": [
    "데이터에 한 가지 이상의 augmentation 방법을 적용하여 학습해 주세요.\n",
    "(어떠한 방법을 사용했는지 적어주세요.)  \n",
    "이전에 구현했던 두 개의 Generator 중 Encoder와 Decoder간에 skip connection이 있는 U-Net Generator를 사용해 주세요.  \n",
    "모델 학습 후, 학습된 Generator를 이용해 테스트합니다. 테스트 데이터는 다운로드했던 \"val\" 폴더 내 이미지를 사용해 주세요.  \n",
    "1개 이상의 이미지에 대해 테스트 과정을 거친 후 그 결과를 스케치, 생성된 사진, 실제 사진 순서로 나란히 시각화해 주세요.  \n",
    "모델을 충분히 학습하기에 시간이 부족할 수 있습니다. 적어도 10 epoch 이상 학습하며 중간 손실 값에 대한 로그를 남겨주세요. 좋은 결과를 얻기 위해선 긴 학습 시간이 필요하므로 테스트 결과는 만족스럽지 않아도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b24e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289f08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42fec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
